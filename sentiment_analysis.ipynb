import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import re
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Parameters
top_words = 10000
max_review_length = 300
embedding_dim = 128
lstm_units = 64
dropout_rate = 0.5
learning_rate = 0.001
batch_size = 64
epochs = 10

# Load the dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

# Pad sequences
X_train = pad_sequences(X_train, maxlen=max_review_length)
X_test = pad_sequences(X_test, maxlen=max_review_length)

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    
    # Handle negations - this is crucial for sentiment analysis
    text = re.sub(r"not\s+(\w+)", r"not_\1", text)
    text = re.sub(r"n't\s+(\w+)", r"not_\1", text)
    
    # Remove non-alphanumeric characters except spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    
    return text

# Build a more efficient model
model = Sequential()
model.add(Embedding(top_words, embedding_dim, input_length=max_review_length))
model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))
model.add(Dropout(dropout_rate))
model.add(GlobalMaxPooling1D())  # More efficient than second LSTM
model.add(Dense(64, activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
optimizer = Adam(learning_rate=learning_rate)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Print model summary
model.summary()

# Define callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_sentiment_model.keras', save_best_only=True, monitor='val_accuracy')

# Train the model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop, model_checkpoint]
)

# Load the best model
model = tf.keras.models.load_model('best_sentiment_model.keras')

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")

# Get word index
word_index = imdb.get_word_index()
index_word = {v+3: k for k, v in word_index.items()}
index_word[0] = "<PAD>"
index_word[1] = "<START>"
index_word[2] = "<UNK>"
index_word[3] = "<UNUSED>"

def encode_review(text):
    # Preprocess the text
    processed_text = preprocess_text(text)
    
    words = processed_text.split()
    encoded = [1]  # <START>
    for word in words:
        if word in word_index and word_index[word] < top_words:
            encoded.append(word_index[word] + 3)
        else:
            encoded.append(2)  # <UNK>
    return pad_sequences([encoded], maxlen=max_review_length)

def predict_sentiment(text):
    encoded = encode_review(text)
    prediction = model.predict(encoded, verbose=0)[0][0]
    # Fixed threshold - use 0.5 for binary classification
    if prediction < 0.5:
        return "Negative", prediction
    else:
        return "Positive", prediction

# Interactive prediction
while True:
    user_review = input("\nEnter your review (type 'exit' to stop): ")
    if user_review.lower() in ["exit", "quit"]:
        print("Session Ended.")
        break
    sentiment, confidence = predict_sentiment(user_review)
    print(f"Predicted Sentiment: {sentiment} (Confidence: {confidence:.2f})")
